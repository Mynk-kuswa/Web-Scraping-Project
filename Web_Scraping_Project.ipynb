{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Web Scraping Project "
      ],
      "metadata": {
        "id": "12JFBycB5mR4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. Pick a website and describe your objective\n",
        "\n",
        "- Browse through different sites and pick on to scrape. Check the \"Project Ideas\" section for inspiration.\n",
        "- Identify the information you'd like to scrape from the site. Decide the format of the output CSV file.\n",
        "- Summarize your project idea and outline your strategy in a Juptyer notebook. Use the \"New\" button above.\n"
      ],
      "metadata": {
        "id": "1llzgwIj44zs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Use the requests library to download web pages\n",
        "\n",
        "- Inspect the website's HTML source and identify the right URLs to download.\n",
        "- Download and save web pages locally using the requests library.\n",
        "- Create a function to automate downloading for different topics/search queries."
      ],
      "metadata": {
        "id": "qPge3-0l5GbR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Use Beautiful Soup to parse and extract information\n",
        "\n",
        "- Parse and explore the structure of downloaded web pages using Beautiful soup.\n",
        "- Use the right properties and methods to extract the required information.\n",
        "- Create functions to extract from the page into lists and dictionaries.\n",
        "(Optional) Use a REST API to acquire additional information if required."
      ],
      "metadata": {
        "id": "LORhZjdi5MUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. Create CSV file(s) with the extracted information\n",
        "\n",
        "- Create functions for the end-to-end process of downloading, parsing, and saving CSVs.\n",
        "- Execute the function with different inputs to create a dataset of CSV files.\n",
        "- Verify the information in the CSV files by reading them back using Pandas."
      ],
      "metadata": {
        "id": "w8qLaHOY5WNO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5. Document and share your work\n",
        "\n",
        "- Add proper headings and documentation in your Jupyter notebook.\n",
        "Publish your Jupyter notebook to your Jovian profile\n",
        "- (Optional) Write a blog post about your project and share it online."
      ],
      "metadata": {
        "id": "LCSNjUrV5cTC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------------------------------------------------------------------------------------------------------------\n",
        "##Steps to follow in this project \n",
        "\n",
        "1. Get the list of topics from topics page\n",
        "2. Get the list of top repos from the individual topics page\n",
        "3. For each topic create a csv file of top repos of topics\n"
      ],
      "metadata": {
        "id": "ZQ9xvhjj7YZ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "outline --\n",
        "\n",
        "- We're going to scrape https://github.com/topics\n",
        "- We'll get a list of topics. For each topics, we'll get topic name, topic title, topic page url and topic stars.\n",
        "- For each repository, we'll grab the repo name, repo username, stars and repo URL.\n",
        "- For each topic we'll create a CSV file in the following formate.\n",
        "\n",
        "\n",
        "```\n",
        "Repo Name,Repo Name,Repo Name,Repo URL\n",
        "three.js,mrdoob,86800,https://github.com/mrdoob/three.js\n",
        "libgdx,libgdx,20700,https://github.com/libgdx/libgdx\n",
        "```"
      ],
      "metadata": {
        "id": "Obcqv7TV57VD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QlntbjyuwlDC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26523c0d-d78e-484e-d941-b2a9c906034f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |█████▏                          | 10 kB 20.0 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 20 kB 22.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 30 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 40 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 51 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 61 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 62 kB 1.1 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# download required libraries\n",
        "\n",
        "!pip install requests --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4 --upgrade --quiet"
      ],
      "metadata": {
        "id": "15Kf5SW0-8ZA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46fb6ad6-cee2-4ea0-d49d-d9f8bebc1e7d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20 kB 19.6 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 30 kB 24.7 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 40 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 51 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 61 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 71 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 81 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 92 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 102 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 112 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 122 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 128 kB 8.5 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "id": "kcUUrF2DwvE_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 1:-  "
      ],
      "metadata": {
        "id": "M81E_nBG5iZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topic_info_df(doc):\n",
        "\n",
        "\n",
        "    topic_title = []\n",
        "    topic_description = []\n",
        "    topic_url = []\n",
        "\n",
        "    # topic _title\n",
        "    selection_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
        "    topic_title_tag = doc.find_all('p', {'class': selection_class})\n",
        "    for tag in topic_title_tag:\n",
        "        topic_title.append(tag.text)\n",
        "\n",
        "    # topic description\n",
        "    selection_class = 'f5 color-fg-muted mb-0 mt-1'\n",
        "    topic_description_tag = doc.find_all('p', {'class': selection_class})\n",
        "    for tag in topic_description_tag:\n",
        "        topic_description.append(tag.text.strip())\n",
        "\n",
        "    # topic url\n",
        "    for urls in topic_title_tag:   \n",
        "        topic_url.append(\"https://github.com\" + urls.parent.get('href'))\n",
        "\n",
        "    # topic details dictionary \n",
        "    topic_dict = {'title': topic_title, 'description': topic_description, \"url\": topic_url} \n",
        "    # topic details DataFrame \n",
        "    topic_df = pd.DataFrame(topic_dict)  \n",
        "\n",
        "    return topic_df   "
      ],
      "metadata": {
        "id": "W9fSLDEBIEWn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get topic page\n",
        "def topic_page():\n",
        "    topic_url = \"https://github.com/topics\"\n",
        "    response = requests.get(topic_url)\n",
        "    page_contents = response.text   \n",
        "    doc = BeautifulSoup(page_contents, 'html.parser')\n",
        "\n",
        "    return doc"
      ],
      "metadata": {
        "id": "WA-krVRi-8Xk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_topics_df():\n",
        "    doc = topic_page()\n",
        "    return get_topic_info_df(doc)"
      ],
      "metadata": {
        "id": "sCN0J8QP3P2C"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 2:-"
      ],
      "metadata": {
        "id": "IMN7rop35maL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def function_1(topic_url):\n",
        "\n",
        "    # loading  data from topic_url \n",
        "    topic_page = requests.get(topic_url)\n",
        "    doc = BeautifulSoup(topic_page.text, 'html.parser')\n",
        "\n",
        "    # dictionary to store all top repository info \n",
        "    dic = {\"repo_name\": [], \"repo_username\": [], \"star_count\": [], \"urls\": []}\n",
        "\n",
        "    # h3_tags contiain repo name , repo_username and urls\n",
        "    parent_class = 'f3 color-fg-muted text-normal lh-condensed'\n",
        "    h3_tags = doc.find_all('h3', {'class' : parent_class})\n",
        "\n",
        "    # star_tag contain star_count for repo\n",
        "    star_class = 'Counter js-social-count'\n",
        "    star_tag = doc.find_all('span', {'class': star_class })\n",
        "\n",
        "    # iterating all top repos for given topic and store it in dictionary (dic)\n",
        "    for i in range(len(star_tag)):\n",
        "        # function_2 is used to take out info from h3_tags nas star_tag\n",
        "        # then append data into dic\n",
        "        repo_info = function_2(h3_tags[i], star_tag[i])                  ##### use of function_2 in function_1\n",
        "        dic['repo_name'].append(repo_info[0])\n",
        "        dic['repo_username'].append(repo_info[1])\n",
        "        dic['star_count'].append(repo_info[2])\n",
        "        dic['urls'].append(repo_info[3])\n",
        "\n",
        "    # convert dic into DataFrame\n",
        "    dic_df = pd.DataFrame(dic)\n",
        "\n",
        "    return dic_df\n",
        "\n",
        "def function_2(h3_tags, star_tag):\n",
        "\n",
        "    base_url = \"https://github.com\"\n",
        "    \n",
        "    repo_name = h3_tags.find_all('a')[0].text.strip()\n",
        "    repo_username = h3_tags.find_all('a')[1].text.strip()\n",
        "    stars = star_tag.text\n",
        "    links = base_url + h3_tags.find_all('a')[1]['href']\n",
        "\n",
        "    return repo_name, repo_username, stars, links"
      ],
      "metadata": {
        "id": "bIp5uQ4b7X9l"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 3:-"
      ],
      "metadata": {
        "id": "zfkuF6vD52E8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Put is all together\n",
        "\n",
        "def scrape_topic(topic_url, path ):\n",
        "    if os.path.exists(path):\n",
        "        print(\"The file path {} already exists. Skipping....\".format(path))\n",
        "        return\n",
        "\n",
        "    topic_repo_df = function_1(topic_url)\n",
        "    topic_repo_df.to_csv(path, index=None)\n",
        "\n",
        "\n",
        "def scrape_topic_repos():\n",
        "    print(\"Scraping list of topics.\")\n",
        "    topics_df = scrape_topics_df()\n",
        "    \n",
        "    os.makedirs(\"data\", exist_ok = True)\n",
        "    for index, row in topics_df.iterrows():\n",
        "        print(\"Scraping top repositories for {}\".format(row['title']))\n",
        "        scrape_topic(row['url'], \"data/{}.csv\".format(row['title']))\n",
        "\n",
        "scrape_topic_repos()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nAnjnUcqHsE",
        "outputId": "67aee61a-f49b-4b23-e265-a972a1563622"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping list of topics.\n",
            "Scraping top repositories for 3D\n",
            "Scraping top repositories for Ajax\n",
            "Scraping top repositories for Algorithm\n",
            "Scraping top repositories for Amp\n",
            "Scraping top repositories for Android\n",
            "Scraping top repositories for Angular\n",
            "Scraping top repositories for Ansible\n",
            "Scraping top repositories for API\n",
            "Scraping top repositories for Arduino\n",
            "Scraping top repositories for ASP.NET\n",
            "Scraping top repositories for Atom\n",
            "Scraping top repositories for Awesome Lists\n",
            "Scraping top repositories for Amazon Web Services\n",
            "Scraping top repositories for Azure\n",
            "Scraping top repositories for Babel\n",
            "Scraping top repositories for Bash\n",
            "Scraping top repositories for Bitcoin\n",
            "Scraping top repositories for Bootstrap\n",
            "Scraping top repositories for Bot\n",
            "Scraping top repositories for C\n",
            "Scraping top repositories for Chrome\n",
            "Scraping top repositories for Chrome extension\n",
            "Scraping top repositories for Command line interface\n",
            "Scraping top repositories for Clojure\n",
            "Scraping top repositories for Code quality\n",
            "Scraping top repositories for Code review\n",
            "Scraping top repositories for Compiler\n",
            "Scraping top repositories for Continuous integration\n",
            "Scraping top repositories for COVID-19\n",
            "Scraping top repositories for C++\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"# Web-Scraping-Project\" >> README.md\n",
        "!git init"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7O_8PVoQ7X5D",
        "outputId": "4d57c5ed-366c-435a-a540-00561ac745cd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reinitialized existing Git repository in /content/.git/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"Mynk-kuswa\"\n",
        "!git config --global user.email \"mayankkushwah912@gmail.com\""
      ],
      "metadata": {
        "id": "DXdwblM47bX1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git init"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13Zlrjc4DaVR",
        "outputId": "683d3036-196d-411d-b4f5-6d527dc52e2e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reinitialized existing Git repository in /content/.git/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git add README.md"
      ],
      "metadata": {
        "id": "5Z3dnB6GEFVW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -m \"first commit\"\n",
        "!git branch -M main\n",
        "!git remote add origin https://github.com/Mynk-kuswa/Web-Scraping-Project.git\n",
        "!git push -u origin main"
      ],
      "metadata": {
        "id": "-TCAvXnqEHjU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}